{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAQPGFJRgPXVP8fIf2c5/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wahiba275/Extract_text_from_YOUTUBE_using_LangChain/blob/main/lang_youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw3WSyBLHHrL",
        "outputId": "84e090fb-5028-4dca-b7b2-866ac4f14780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.343-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.7 (from langchain)\n",
            "  Downloading langchain_core-0.0.7-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, langsmith, jsonpatch, httpcore, langchain-core, httpx, dataclasses-json, openai, langchain\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.343 langchain-core-0.0.7 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.3.6 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "07c7Ot5vIB10",
        "outputId": "0110a99f-abfb-4325-e749-a7446a9c63ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.3.6\n",
            "    Uninstalling openai-1.3.6:\n",
            "      Successfully uninstalled openai-1.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install yt_dlp\n",
        "! pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdlfKrKhHKcc",
        "outputId": "17543155-e270-4677-ec78-7260ae8e5da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2023.11.16-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt_dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt_dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt_dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2023.7.22)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt_dlp) (2.0.7)\n",
            "Collecting brotli (from yt_dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt_dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt_dlp) (3.4)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt_dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-12.0 yt_dlp-2023.11.16\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key='your openai key'"
      ],
      "metadata": {
        "id": "_3GRWSHKHRQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade\n",
        "!openai migrate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ez_lZ1KKdc",
        "outputId": "4b386c00-2950-442d-de7d-6781b6e912ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Collecting openai\n",
            "  Using cached openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-1.3.6\n",
            "Retrieving Grit CLI metadata from https://api.keygen.sh/v1/accounts/custodian-dev/artifacts/marzano-linux-x64\n",
            "\n",
            "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m                                                                      \u001b[0m\n",
            "\u001b[2K\u001b[2A\n",
            "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m                                                                      \u001b[0m\n",
            "\u001b[2K\u001b[2A\n",
            "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m                                                                      \u001b[0m\n",
            "\u001b[2K\u001b[2A\n",
            "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m                                                                      \u001b[0m\n",
            "\u001b[2K\u001b[2A\n",
            "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m                                                                      \u001b[0m\n",
            "\u001b[2K\u001b[2AProcessed 0 files and found 0 matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osoXbKIQNTQ4",
        "outputId": "53fdd695-af8a-4431-c51d-51593c022166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
        "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
      ],
      "metadata": {
        "id": "_IBPOMVUHRTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "ZBUz0vVjJ34k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y62wSXmwNnbX",
        "outputId": "6c1fcec3-c6b5-4c4a-ff1d-ce441ad39999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2023.7.22)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader"
      ],
      "metadata": {
        "id": "p47FRBmwNk_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=xUNx_PxNHrY&t=3s&ab_channel=TED\", add_video_info=True\n",
        ")\n",
        "loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk6vyqV_HRV8",
        "outputId": "ca1a9cfc-8e0b-4bb3-9105-a00ce661b713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Five years ago, I stood on the TED stage and warned about the dangers\\nof superintelligence. I was wrong. It went even worse than I thought. (Laughter) I never thought governments\\nwould let AI companies get this far without any meaningful regulation. And the progress of AI\\nwent even faster than I predicted. Look, I showed this abstract\\nlandscape of tasks where the elevation represented\\nhow hard it was for AI to do each task at human level. And the sea level represented\\nwhat AI could be back then. And boy or boy, has the sea\\nbeen rising fast ever since. But a lot of these tasks have already\\ngone blub blub blub blub blub blub. And the water is on track\\nto submerge all land, matching human intelligence\\nat all cognitive tasks. This is a definition of artificial\\ngeneral intelligence, AGI, which is the stated goal\\nof companies like OpenAI, Google DeepMind and Anthropic. And these companies are also trying\\nto build superintelligence, leaving human intelligence far behind. And many think it\\'ll only be a few years,\\nmaybe, from AGI to superintelligence. So when are we going to get AGI? Well, until recently, most AI researchers\\nthought it was at least decades away. And now Microsoft is saying,\\n\"Oh, it\\'s almost here.\" We\\'re seeing sparks of AGI in ChatGPT-4, and the Metaculus betting site\\nis showing the time left to AGI plummeting from 20 years away\\nto three years away in the last 18 months. And leading industry people\\nare now predicting that we have maybe two or three years left\\nuntil we get outsmarted. So you better stop talking\\nabout AGI as a long-term risk, or someone might call you a dinosaur\\nstuck in the past. It\\'s really remarkable\\nhow AI has progressed recently. Not long ago, robots moved like this. (Music) Now they can dance. (Music) Just last year, Midjourney\\nproduced this image. This year, the exact\\nsame prompt produces this. Deepfakes are getting really convincing. (Video) Deepfake Tom Cruise:\\nI’m going to show you some magic. It\\'s the real thing. (Laughs) I mean ... It\\'s all ... the real ... thing. Max Tegmark: Or is it? And Yoshua Bengio now argues that large language models\\nhave mastered language and knowledge to the point\\nthat they pass the Turing test. I know some skeptics are saying, \"Nah, they\\'re just overhyped\\nstochastic parrots that lack a model of the world,\" but they clearly have\\na representation of the world. In fact, we recently found that Llama-2\\neven has a literal map of the world in it. And AI also builds geometric representations\\nof more abstract concepts like what it thinks is true and false. So what\\'s going to happen\\nif we get AGI and superintelligence? If you only remember one thing\\nfrom my talk, let it be this. AI godfather, Alan Turing predicted that the default outcome\\nis the machines take control. The machines take control. I know this sounds like science fiction, but, you know, having AI as smart as GPT-4 also sounded like science\\nfiction not long ago. And if you think of AI, if you think of superintelligence\\nin particular, as just another technology, like electricity, you\\'re probably not very worried. But you see, Turing thinks of superintelligence\\nmore like a new species. Think of it, we are building creepy, super capable, amoral psychopaths that don\\'t sleep and think\\nmuch faster than us, can make copies of themselves and have nothing human about them at all. So what could possibly go wrong? (Laughter) And it\\'s not just Turing. OpenAI CEO Sam Altman,\\nwho gave us ChatGPT, recently warned that it could\\nbe \"lights out for all of us.\" Anthropic CEO, Dario Amodei,\\neven put a number on this risk: 10-25 percent. And it\\'s not just them. Human extinction from AI\\nwent mainstream in May when all the AGI CEOs\\nand who\\'s who of AI researchers came on and warned about it. And last month, even the number one\\nof the European Union warned about human extinction by AI. So let me summarize\\neverything I\\'ve said so far in just one slide of cat memes. Three years ago, people were saying it\\'s inevitable,\\nsuperintelligence, it\\'ll be fine, it\\'s decades away. Last year it was more like, It\\'s inevitable, it\\'ll be fine. Now it\\'s more like, It\\'s inevitable. (Laughter) But let\\'s take a deep breath\\nand try to raise our spirits and cheer ourselves up, because the rest of my talk\\nis going to be about the good news, that it\\'s not inevitable,\\nand we can absolutely do better, alright? (Applause) So ... The real problem is that we lack\\na convincing plan for AI safety. People are working hard on evals looking for risky AI behavior,\\nand that\\'s good, but clearly not good enough. They\\'re basically training AI\\nto not say bad things rather than not do bad things. Moreover, evals and debugging\\nare really just necessary, not sufficient, conditions for safety. In other words, they can prove the presence of risk, not the absence of risk. So let\\'s up our game, alright? Try to see how we can make\\nprovably safe AI that we can control. Guardrails try to physically limit harm. But if your adversary is superintelligence or a human using superintelligence\\nagainst you, right, trying is just not enough. You need to succeed. Harm needs to be impossible. So we need provably safe systems. Provable, not in the weak sense\\nof convincing some judge, but in the strong sense of there being\\nsomething that\\'s impossible according to the laws of physics. Because no matter how smart an AI is, it can\\'t violate the laws of physics\\nand do what\\'s provably impossible. Steve Omohundro and I\\nwrote a paper about this, and we\\'re optimistic\\nthat this vision can really work. So let me tell you a little bit about how. There\\'s a venerable field\\ncalled formal verification, which proves stuff about code. And I\\'m optimistic that AI will\\nrevolutionize automatic proving business and also revolutionize program synthesis, the ability to automatically\\nwrite really good code. So here is how our vision works. You, the human, write a specification that your AI tool must obey, that it\\'s impossible to log in\\nto your laptop without the correct password, or that a DNA printer\\ncannot synthesize dangerous viruses. Then a very powerful AI\\ncreates both your AI tool and a proof that your tool\\nmeets your spec. Machine learning is uniquely good\\nat learning algorithms, but once the algorithm has been learned, you can re-implement it in a different\\ncomputational architecture that\\'s easier to verify. Now you might worry, how on earth am I going\\nto understand this powerful AI and the powerful AI tool it built and the proof, if they\\'re all too complicated\\nfor any human to grasp? Here is the really great news. You don\\'t have to understand\\nany of that stuff, because it\\'s much easier to verify\\na proof than to discover it. So you only have to understand\\nor trust your proof-checking code, which could be just\\na few hundred lines long. And Steve and I envision that such proof checkers get built\\ninto all our compute hardware, so it just becomes impossible\\nto run very unsafe code. What if the AI, though,\\nisn\\'t able to write that AI tool for you? Then there\\'s another possibility. You train an AI to first just learn\\nto do what you want and then you use a different AI to extract out the learned algorithm\\nand knowledge for you, like an AI neuroscientist. This is in the spirit of the field\\nof mechanistic interpretability, which is making really\\nimpressive rapid progress. Provably safe systems\\nare clearly not impossible. Let\\'s look at a simple example of where we first machine-learn\\nan algorithm from data and then distill it out\\nin the form of code that provably meets spec, OK? Let’s do it with an algorithm\\nthat you probably learned in first grade, addition, where you loop over the digits\\nfrom right to left, and sometimes you do a carry. We\\'ll do it in binary, as if you were counting\\non two fingers instead of ten. And we first train a recurrent\\nneural network, never mind the details, to nail the task. So now you have this algorithm\\nthat you don\\'t understand how it works in a black box defined by a bunch of tables\\nof numbers that we, in nerd speak, call parameters. Then we use an AI tool we built\\nto automatically distill out from this the learned algorithm\\nin the form of a Python program. And then we use the formal\\nverification tool known as Daphne to prove that this program\\ncorrectly adds up any numbers, not just the numbers\\nthat were in your training data. So in summary, provably safe AI,\\nI\\'m convinced is possible, but it\\'s going to take time and work. And in the meantime, let\\'s remember that all the AI benefits that most people are excited about actually don\\'t require superintelligence. We can have a long\\nand amazing future with AI. So let\\'s not pause AI. Let\\'s just pause the reckless\\nrace to superintelligence. Let\\'s stop obsessively training\\never-larger models that we don\\'t understand. Let\\'s heed the warning from ancient Greece and not get hubris,\\nlike in the story of Icarus. Because artificial intelligence is giving us incredible intellectual wings with which we can do things\\nbeyond our wildest dreams if we stop obsessively\\ntrying to fly to the sun. Thank you. (Applause)', metadata={'source': 'xUNx_PxNHrY', 'title': 'How to Keep AI Under Control | Max Tegmark | TED', 'description': 'Unknown', 'view_count': 109811, 'thumbnail_url': 'https://i.ytimg.com/vi/xUNx_PxNHrY/hq720.jpg', 'publish_date': '2023-11-02 00:00:00', 'length': 730, 'author': 'TED'})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}